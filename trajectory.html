<!DOCTYPE html><!--[if lt IE 9]><html class="ie"><![endif]--><html lang="en"><head><meta charset="UTF-8"><title>Apollo Scape</title><meta name="keywords" content="Apollo, Scape, Apollo Scape"><meta name="description" content="Baidu Apollo Scape"><meta name="viewport" content="width=device-width"><link rel="shortcut icon" href="/public/img/common/favicon.ico"><link rel="stylesheet" type="text/css" href="/css/common/base_23ab8c6.css"><link rel="stylesheet" type="text/css" href="/css/common/custom_23c4c53.css"><script type="text/javascript" src="/js/lib/jquery-3.2.1.min_34c3e57.js"></script><script type="text/javascript" src="/js/lib/jquery.cookie_801d29b.js"></script><script type="text/javascript" src="/js/lib/vue.min_da55047.js"></script><script type="text/javascript" src="/js/lib/vue-i18n.min_26d2756.js"></script><script type="text/javascript" src="/js/lib/tool_72d9a64.js"></script><script>var _hmt;
_hmt = _hmt || [];
(function() {
    var hm = document.createElement('script');
    hm.src = 'https://hm.baidu.com/hm.js?54a6c097c87d817f40d099a1b48226f0';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(hm, s);
}) ();</script><link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/trajectory/index_ba40f8d.css"><link rel="stylesheet" type="text/css" media="screen and (max-width: 750px)" href="/css/trajectory/m-index_d41d8cd.css"><link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/scene/github_37a6c24.css"><link rel="stylesheet" type="text/css" media="screen and (min-width: 750px)" href="/css/scene/scene_f53055b.css"><style type="text/css">.scene-body .content .github_table{border:unset;height:unset}.markdown-body p,.markdown-body blockquote,.markdown-body ul,.markdown-body ol,.markdown-body dl,.markdown-body table,.markdown-body pre{font-size:14px;color:#666}.markdown-body .title-1{font-size:20px;color:#333}.markdown-body .mt60{margin-top:60px!important}.scene-body .content{padding-left:25px}.tip-top{word-wrap:normal}ol li{list-style-type:decimal}ul.submission-ul li{list-style-type:disc}.word-1.indent{padding-left:1.2em}</style></head><body><header class="header main-wrapper"><div class="main clearfix"><a href="/index.html" class="logo fl sub-nav-flag"><img src="/public/img/common/logo_a7cf1b2.png"></a><div class="nav-list-mask"></div><ul class="nav-list fl"></ul><div class="language fr sub-nav-flag"><div class="fl" id="lang-login-container"></div></div><div class="cb sub-nav-flag"></div><div class="sub-nav"><div class="sub-nav-content"><ul class="fr" id="sub-nav-ul"></ul></div></div><div class="nav-prompt fr"><span class="texts fl" id="use_pc_text"></span><span class="mobile-banner-close fr"></span></div><script type="text/javascript" src="/js/nav_f9c19e3.js"></script></div></header><section class="banner main-wrapper scene-banner" id="banner-container"></section><section class="main-wrapper"><div class="main scene-body" id="scene-container"><div class="menu fl"></div><div class="content fl"><p class="title-1 mt60" id="to_data_href">1 · Introduction</p><p class="word-1 mt20" id="to_collection_href">We use Apollo acquisition car to collect trafﬁc data, including camera-based images and LiDAR based point clouds, and generate trajectories by high quality annotation. We collected the trajectory dataset in Beijing, consisting of varying light conditions and trafﬁc densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another.</p><div class="fl mt20 demo"><img src="/public/img/trajectory/traj-seq_f5b7aaf.gif" width="500px"></div><div class="cb"></div><p class="title-1 mt60" id="to_download_href">2 · Data Download</p><p class="word-1 mt10">The trajectory prediction benchmark consists of 53min training sequences and 50min testing sequences. The data we provide is 2fps</p><div class="down_list mt20 clearfix"><a class="down_btn fl mr10" value="prediction_test.zip">prediction_test.zip</a></div><div class="down_list mt20 clearfix"><a class="down_btn fl mr10" value="prediction_train.zip">prediction_train.zip</a></div><div class="cb"></div><div class="cb"></div><p class="title-1 mt60" id="to_structure_href">3 · Data Structure</p><p class="word-1 mt10">The folder structure of the trajectory prediction is as follows:</p><p class="word-1 mt10">1) prediction_train.zip: training data for trajectory prediction.</p><p class="word-1 indent">∙ Each file is a 1min sequence with 2fps.<br>∙ Each line in every file contains frame_id, object_id, object_type, position_x, position_y, position_z, object_length, object_width, object_height, heading.<br>∙ For object_type, 1 for small vehicles, 2 for big vehicles, 3 for pedestrian, 4 for bicyclist and 5 for others. We consider the first two types as one type (vehicles) in this challenge.<br>∙ Position is in the world coordinate system. We just think the traffic in a 2D plane and ignore its position_z. The unit for the position and bounding box is meter.<br>∙ The heading value is the steering radian with respect to the direction of the object.<br>∙ You do not need to use all the info we provide. We just consider your predicted position_x and position_y in the next several seconds.</p><p class="word-1 mt10">2) prediction_test.zip: testing data for trajectory prediction.</p><p class="word-1 indent">∙ Each line contains frame_id, object_id, object_type, position_x, position_y, position_z, object_length, object_width, object_height, heading.<br>∙ Every six frames in the prediction_test.txt is a testing sequence. Each sequence is independent. You need read the file carefully.</p><p class="title-1 mt60" id="to_scripts_href">4 · Scripts</p><p class="word-1 mt20">The evaluation scripts are released on github <a href="#">here</a>.</p><p class="title-1 mt60" id="to_evaluation_href">5 · Evaluation</p><p class="word-1 mt20">Once you want to test your method on the test set, please run your approach on the provided test dataset and submit your results to our Challenge.</p><p class="word-1 mt20">We have labeled five types of instances in our dataset: 1 for small vehicles, 2 for big vehicles, 3 for pedestrian, 4 for bicyclist and 5 for others. However, we consider the first two types as one type (vehicles) in this challenge. We do evaluation just for vehicles, pedestrian and bicyclist. The setting for this problem is to observe 3s (6 positions) and predict trajectories for the following 3s (6 positions). We think the objects in the last frame of observation as considered objects and compare the error between your predicted locations and the ground truth for these considered objects.</p><div class="cb"></div><p class="title-1 mt60" id="to_metric_href">6 · Metric formula</p><p class="word-1 mt10 markdown-body">We use the following metrics[1] to measure the performance of algorithms used for predicting the trajectories of each type of objects.</p><p class="word-1 mt10 markdown-body">1. Average displacement error (ADE): The mean Euclidean distance over all the predicted positions and real positions during the prediction time.</p><p class="word-1 mt10 markdown-body">2. Final displacement error (FDE): The mean Euclidean distance between the final predicted positions and the corresponding true locations.<br>Because the trajectories of cars, bicyclist and pedestrians have different scales, we use the following weighted sum of ADE (WSADE) and weighted sum of FDE (WSFDE) as metrics.</p><div class="cb"></div><img src="/public/img/scene/prediction1_c0801d7.png" align="middle" width="300px" style="max-width:100%"><p class="word-1 mt10 markdown-body">where <img class="spce-icon" src="/public/img/scene/prediction_img/DV_a927731.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%">, <img class="spce-icon extra" src="/public/img/scene/prediction_img/DP_cdd1e95.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%">, and <img class="spce-icon" src="/public/img/scene/prediction_img/DB_a4b7633.png" align="middle" width="12.067218899999991pt" height="14.15524440000002pt/" style="max-width:100%"> are related to reciprocals of the average velocity of vehicles, pedestrian and bicyclist in the dataset. We adopt 0.20, 0.58, 0.22 respectively.</p><div class="cb"></div><p class="title-1 mt60" id="to_rules_href">7 · Rules of ranking</p><p class="word-1 mt10">Result benchmark will be:</p><div class="markdown-body"><table class="github_table"><thead><tr><th>Rank</th><th>Method</th><th align="center">WSADE</th><th align="center">ADEv</th><th align="center">ADEp</th><th align="center">ADEb</th><th align="center">WSFDE</th><th align="center">FDEv</th><th align="center">FDEp</th><th align="center">FDEb</th><th>Team Name</th></tr></thead><tbody><tr><td align="center">xx</td><td>xxx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td><td align="center">xx</td></tr></tbody></table></div><p class="word-1 mt10">Our ranking will determined by WSADE of all types of objects.</p><div class="cb"></div><p class="title-1 mt60" id="to_format_href">8 · Format of submission file</p><p class="word-1 mt10">You just need to submit a prediction_result.txt.<br>- Each line contains frame_id, object_id, object_type, position_x, and position_y in order.<br>- We think every six frames as a predicted sequence result for a sequence in the test data. Pay attention to make right correspondence to test data. It means sequences in test data and your result should have the same number and same order. Same objects should have the same id. Different frames should have different ids.</p><div class="cb"></div><div class="eccv"><a class="task-submission-btn submit_achievement">Participate</a> <a class="task-submission-btn leader_board" href="/ECCV/trajectoryboard.html" target="_Blank">LeaderBoard</a></div><div class="cb"></div><p class="title-1 mt60" id="to_publicat_href">9 · Publication</p><table style="width:100%" cellspacing="10" valign="top"><tr><td valign="top" align="left"><p>Please cite our paper in your publications if our dataset is used in your research.</p><p>TrafficPredict: Trajectory Prediction for Heterogeneous Traffic-Agents [<a href="https://arxiv.org/pdf/1811.02146.pdf"><font color="red">PDF</font></a>]<br>Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, and Dinesh Manocha.<br><em>AAAI(oral), 2019</em><br><br></p></td></tr></table><div class="cb"></div><p class="title-1 mt60" id="to_reference_href">10 · Reference</p><p class="word-1 mt10">[1] Pellegrini S, Ess A, Schindler K, et al. You'll never walk alone: Modeling social behavior for multi-target tracking[C]. Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009: 261-268.</p><div class="cb"></div><article class="markdown-body entry-content" itemprop="text"><p class="title-1 mt60" id="to_contact_href">Contact</p><p>Please feel free to contact us with any questions, suggestions or comments:</p><p><a href="mailto:apollo-scape@baidu.com">apollo-scape@baidu.com</a></p></article><div class="mt60" style="font-size:18px;text-align:center">The dataset we released is &nbsp;desensitized street view for academic use only.</div></div><div class="cb"></div></div></section><div class="protocol-bg" id="protocol"></div><div class="protocol_container" id="protocol_container"></div><div class="done_load"></div><script type="text/javascript" src="/js/dataset_trajectory_ce4e95a.js"></script><footer class="footer main-wrapper"><div class="footer-link tc fl"></div><p class="copyright fl" id="copyright"></p><ul class="tooltip"><li id="tip-top" class="tip-item tip-top" style="display:list-item">·µ»Ø¶¥²¿</li><li class="tip-item" style="display:list-item">联系我们</li></ul></footer><script type="text/javascript" src="/js/footer_d34fe6b.js"></script></body></html>